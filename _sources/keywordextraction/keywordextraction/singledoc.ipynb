{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keywordExtraction.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdWZnkZksb0S"
      },
      "source": [
        "# Tfidf and RAKE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIyVa8GrssOz"
      },
      "source": [
        "## Downdload dataset: [Inspec](https://dl.acm.org/doi/10.3115/1119355.1119383)\n",
        "\n",
        "Inspec consists of 2,000 abstracts of scientific journal papers from Computer Science collected between the years 1998 and 2002. Each document has two sets of keywords assigned: the controlled keywords, which are manually controlled assigned keywords that appear in the Inspec thesaurus but may not appear in the document, and the uncontrolled keywords which are freely assigned by the editors, i.e., are not restricted to the thesaurus or to the document. In our repository, we consider a union of both sets as the ground-truth .[[Download source]](https://github.com/LIAAD/KeywordExtractor-Datasets#inspec)"
      ]
    },
    {
      "source": [
        "!wget https://github.com/LIAAD/KeywordExtractor-Datasets/raw/master/datasets/Inspec.zip"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "_HMOgHRg7Q6_"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-02-02 10:27:09--  https://github.com/LIAAD/KeywordExtractor-Datasets/raw/master/datasets/Inspec.zip\n",
            "Resolving github.com (github.com)... 52.192.72.89\n",
            "Connecting to github.com (github.com)|52.192.72.89|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/LIAAD/KeywordExtractor-Datasets/master/datasets/Inspec.zip [following]\n",
            "--2021-02-02 10:27:09--  https://raw.githubusercontent.com/LIAAD/KeywordExtractor-Datasets/master/datasets/Inspec.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.108.133\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1890788 (1.8M) [application/zip]\n",
            "Saving to: â€˜Inspec.zip.3â€™\n",
            "\n",
            "Inspec.zip.3        100%[===================>]   1.80M  3.37MB/s    in 0.5s    \n",
            "\n",
            "2021-02-02 10:27:10 (3.37 MB/s) - â€˜Inspec.zip.3â€™ saved [1890788/1890788]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!unzip -q Inspec.zip"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "hcQUcp257dxE"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace Inspec/docsutf8/114.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oXLN2Pk74b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bc7ae3f-b97e-4009-b747-30f56045e136"
      },
      "source": [
        "from os import listdir\n",
        "\n",
        "docsPaths = listdir('Inspec/docsutf8')\n",
        "data = []\n",
        "for p in docsPaths:\n",
        "  textPath = 'Inspec/docsutf8/'+p\n",
        "  keyPath = 'Inspec/keys/'+p[:-3]+'key'\n",
        "  with open(textPath) as fi:\n",
        "    text = fi.read()\n",
        "  with open(keyPath) as fi:\n",
        "    key = fi.read()\n",
        "  data.append({'text':text, 'key':key})\n",
        "print(\"make data: DONE\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "make data: DONE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUN91JS4ta1e",
        "outputId": "491693c1-40bd-46b2-f2f7-87dba52d71f8"
      },
      "source": [
        "print(\"Total number of texts in the dataset:\",len(data))\n",
        "print(\"data[1000]:\\n\\n\",data[1000]['text'])\n",
        "print(\"\\n\\nkeywords:\\n\")\n",
        "for i in data[1000]['key'].replace('\\t','').split(\"\\n\"):\n",
        "  print(i)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of texts in the dataset: 2000\ndata[1000]:\n\n Color plane interpolation using alternating projections\nMost commercial digital cameras use color filter arrays to sample red, green,\n\tand blue colors according to a specific pattern. At the location of\n\teach pixel only one color sample is taken, and the values of the other\n\tcolors must be interpolated using neighboring samples. This color plane\n\tinterpolation is known as demosaicing; it is one of the important tasks\n\tin a digital camera pipeline. If demosaicing is not performed\n\tappropriately, images suffer from highly visible color artifacts. In\n\tthis paper we present a new demosaicing technique that uses\n\tinter-channel correlation effectively in an alternating-projections\n\tscheme. We have compared this technique with six state-of-the-art\n\tdemosaicing techniques, and it outperforms all of them, both visually\n\tand in terms of mean square error\n\n\n\nkeywords:\n\ncolor plane interpolation\nalternating projections\ndigital cameras\ndemosaicing\ncolor filter arrays\ncolor artifacts\ninter-channel correlation\ncameras\ncorrelation methods\nimage colour analysis\ninterpolation\noptical filters\n\n"
          ]
        }
      ]
    },
    {
      "source": [
        "## Text Preprocessing\n",
        "\n",
        "\n",
        "In text preprocessing we : \n",
        "\n",
        "*   convert to lowercase\n",
        "*   remove tages\n",
        "*   remove special characters and digits\n",
        "*   remove [stopwords](https://en.wikipedia.org/wiki/Stop_word)"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoEXE5_Fs19v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/site-packages (3.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/site-packages (from nltk) (2020.11.13)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/site-packages (from nltk) (1.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/site-packages (from nltk) (4.56.0)\n"
          ]
        }
      ],
      "source": [
        "# !pip install nltk  # install nltk if you have not done before"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igGNrEH3-h_z",
        "outputId": "7ed60445-a65d-444d-ddaf-da017719091b"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "#nltk.download('stopwords') # uncomment to download stopwords\n",
        "#nltk.download('wordnet') # uncomment to download wordnet\n",
        "stopwords_set = set(stopwords.words('english'))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/rinmensai/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/rinmensai/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uis3S4YT-sB3"
      },
      "source": [
        "def preprocess(input_text):\n",
        "    input_text = input_text.lower()\n",
        "    input_text = re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \", input_text)\n",
        "    input_text = re.sub(\"(\\\\d|\\\\W)+\",\" \", input_text)\n",
        "    text_string = input_text.split()\n",
        "    text_string = [word for word in text_string if word not in stopwords_set]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text_string = [lemmatizer.lemmatize(word) for word in text_string]\n",
        "    return ' '.join(text_string)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GI-ypX6S-tLS",
        "outputId": "ac3b1271-2309-44c7-bea2-c117374b8b38"
      },
      "source": [
        "docs = [preprocess(x['text']) for x in data]\n",
        "keys = [x['key'].replace('\\t','').split('\\n')[:-1] for x in data]\n",
        "print(len(docs),len(keys))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuByKGZRs5OE"
      },
      "source": [
        "## TF-IDF based method\n",
        "\n",
        "In this method, each word in text(tokenized string list) is consider as a candidate. Score of cadidate keyword is define by [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) score. The k candidate keywords with the highest TF-IDF score are selected as keywords.\n",
        "\n",
        "TF-IDF(term frequencyâ€“inverse document frequency) is a numerical statistic which reflect the importance of word in a text while considering it occurence within a group of documents(corpus).\n",
        "\n",
        "Consider a corpus with $N$ documents. Let $t_{i,j}$ be frequency of term $w_i (i=1,\\cdots,W)$ in document $D_j$ and $df_i$ be number of documents which term $w_i$ occurences in. TF-IDF score of term $w_i$ is defined by $\\operatorname{tfidf}(i)$.\n",
        "\n",
        "$$\n",
        "\\operatorname{tfidf}(i) = \\sum_{j=1}^{N} t_{i,j}\\log{\\frac{N}{df_i}}\n",
        "$$\n",
        "\n",
        "In the following implimentation, we use tools offered by *scikitlearn*.\n",
        "\n",
        "[CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn-feature-extraction-text-countvectorizer)     \n",
        "[TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.8/site-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/site-packages (from sklearn) (0.24.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.19.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/site-packages (from scikit-learn->sklearn) (2.1.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.6.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "# !pip install sklearn # uncomment to install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxRv1szqBYxd"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrwrCRrHBcHQ",
        "outputId": "8590d3c0-ddb2-46f9-98b5-73dc174ecb94"
      },
      "source": [
        "vectorizer = CountVectorizer(max_df=0.95, max_features=20000, ngram_range=(1,4))\n",
        "word_freq_vec = vectorizer.fit_transform(docs)\n",
        "transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
        "transformer.fit(word_freq_vec)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfTransformer()"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zx6WMM0WBfhg"
      },
      "source": [
        "def extract_topk(features, sorted_words, k=10):\n",
        "    sorted_words = sorted_words[:k]\n",
        "    scores = []\n",
        "    f_list = []\n",
        "    for i, score in sorted_words:\n",
        "        scores.append(round(score, 3))\n",
        "        f_list.append(features[i])\n",
        "    result = {}\n",
        "    for i in range(len(f_list)):\n",
        "        result[f_list[i]] = scores[i]\n",
        "    return result\n",
        "\n",
        "def get_keywords(idx, docs):\n",
        "    tfidf_vec = transformer.transform(vectorizer.transform([docs[idx]]))\n",
        "    co_matrix = tfidf_vec.tocoo()\n",
        "    sorted_words = sorted(zip(co_matrix.col, co_matrix.data), key=lambda x: (x[1],x[0]), reverse=True)\n",
        "    features = vectorizer.get_feature_names()\n",
        "    keywords = extract_topk(features, sorted_words, 10)\n",
        "    return keywords\n",
        "\n",
        "def print_keywords(idx, keywords, data, ans):\n",
        "    ans = [preprocess(a) for a in ans[:len(keywords)]]\n",
        "    print(\"\\n=====Abstract=====\")\n",
        "    print(data[idx]['text'])\n",
        "    print(\"\\n===Keywords===\")\n",
        "    for k in keywords:\n",
        "        if k in ans:\n",
        "          print(\"ðŸŸ¢\",end='')\n",
        "        print(k,keywords[k])\n",
        "    print(\"\\n===Keywords by Author===\")\n",
        "    ext_keys = [i for i in keywords]\n",
        "    for i in ans:\n",
        "      if i in ext_keys:\n",
        "          print(\"ðŸ™†â€â™‚ï¸\",end=\"\")\n",
        "      print(i)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f0ZAmpbBjvs",
        "outputId": "202ccf3e-f485-4b66-e401-b80ca85440bd"
      },
      "source": [
        "sample_idx = 238\n",
        "keywords=get_keywords(sample_idx, docs)\n",
        "print_keywords(sample_idx,keywords, data, keys[sample_idx])"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=====Abstract=====\nFractional differentiation in passive vibration control\nFrom a single-degree-of-freedom model used to illustrate the concept of\n\tvibration isolation, a method to transform the design for a suspension\n\tinto a design for a robust controller is presented. Fractional\n\tdifferentiation is used to model the viscoelastic behaviour of the\n\tsuspension. The use of fractional differentiation not only permits\n\toptimisation of just four suspension parameters, showing the\n\t'compactness' of the fractional derivative operator, but also leads to\n\trobustness of the suspension's performance to uncertainty of the sprung\n\tmass. As an example, an engine suspension is studied\n\n\n===Keywords===\nðŸŸ¢suspension 0.537\nfractional 0.353\nðŸŸ¢fractional differentiation 0.341\nðŸŸ¢differentiation 0.289\nvibration 0.164\nðŸŸ¢vibration isolation 0.118\nuse fractional 0.118\nsingle degree freedom 0.118\nsingle degree 0.118\noperator also 0.118\n\n===Keywords by Author===\nðŸ™†â€â™‚ï¸fractional differentiation\npassive vibration control\nðŸ™†â€â™‚ï¸vibration isolation\nðŸ™†â€â™‚ï¸suspension\nrobust controller\nviscoelastic behaviour\nsprung mass\nengine suspension\ndamping\nðŸ™†â€â™‚ï¸differentiation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuyMHTb-Dtia"
      },
      "source": [
        "With this method, four of extracted top ten keywords are in keywords set selected by author. This result is somehow acceptable. However, to calculate TF-IDF score, a collection of documents(corpus) is required as reference data. Therefore, characteristic of reference data(corpus) highly affect the importance of word in single document. Keywords that occur in many documents within the corpus are not likely to be selected as statistically discriminating by lowering the TF-IDF score value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qis1Ijd5tBCD"
      },
      "source": [
        "## RAKE: Rapid Automatic Keyword Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZZhBM9hGwmK"
      },
      "source": [
        "*(Ref: Rose, Stuart & Engel, Dave & Cramer, Nick & Cowley, Wendy. (2010). [Automatic Keyword Extraction from Individual Documents](https://www.researchgate.net/publication/227988510_Automatic_Keyword_Extraction_from_Individual_Documents). 10.1002/9780470689646.ch1.)*\n",
        "\n",
        "\n",
        "\n",
        "RAKE is a keyword extraction method which is effectively operates on individual document. RAKE is based on an observation that keywords frequently contain multiple\n",
        "words but rarely contain standard punctuation or stop words.\n",
        "\n",
        "RAKE get a list of stop words, a\n",
        "set of phrase delimiters, and a set of word delimiters as inputs. Base on the information from stop words and\n",
        "phrase delimiters, RAKE makes partitions of the document text into candidate keywords. \n",
        "\n",
        "Next, co-occurrences graph of words\n",
        "within these candidate keywords are generated. This graph is significient in identifying word cooccurrence without the application of an arbitrarily sized sliding window. \n",
        "\n",
        "Several metrics are used for calculating word scores\n",
        "\n",
        "1.   word frequency : $freq(w)$\n",
        "2.   word degree : $deg(w)$\n",
        "3.   ratio of degree to frequency : $\\frac{deg(w)}{freq(w)}$\n",
        "\n",
        "In summary, $deg(w)$ favors words that occur often and in\n",
        "longer candidate keywords. Words\n",
        "that occur frequently regardless of the number of words with which they co-occur\n",
        "are favored by $freq(w)$. Words that\n",
        "predominantly occur in longer candidate keywords are favored by $deg(w)/freq(w)$ [[Ref]](https://www.researchgate.net/publication/227988510_Automatic_Keyword_Extraction_from_Individual_Documents).\n",
        "\n",
        "The score for each candidate keyword is computed as the sum of its member word scores.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMMZd2PRJ8_r"
      },
      "source": [
        "**Example of score calculation from  [Automatic Keyword Extraction from Individual Documents](https://www.researchgate.net/publication/227988510_Automatic_Keyword_Extraction_from_Individual_Documents)**\n",
        "\n",
        "Figures from Original Paper of [RAKE](https://www.researchgate.net/publication/227988510_Automatic_Keyword_Extraction_from_Individual_Documents)\n",
        "\n",
        "![fig 01](fig01.png)\n",
        "\n",
        "![fig 02](fig02.png)\n",
        "\n",
        "![fig 03](fig03.png)\n",
        "\n",
        "![fig 04](fig04.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNYq81jqCVgu",
        "outputId": "26d13f07-615d-430a-dfe5-b3c2bfe50858"
      },
      "source": [
        "# !pip install rake-nltk # uncomment to install rake-nltk"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rake-nltk in /usr/local/lib/python3.8/site-packages (1.0.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/site-packages (from rake-nltk) (3.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/site-packages (from nltk->rake-nltk) (4.56.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/site-packages (from nltk->rake-nltk) (2020.11.13)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/site-packages (from nltk->rake-nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/site-packages (from nltk->rake-nltk) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMQ5BQHKG1A6"
      },
      "source": [
        "from rake_nltk import Rake"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rQ85qEmG2gW"
      },
      "source": [
        "def get_keywords_rake(docs, n=10):\n",
        "    rk = Rake()\n",
        "    rk.extract_keywords_from_text(docs)\n",
        "    keywords = rk.get_ranked_phrases_with_scores()[0:n]\n",
        "    return keywords\n",
        "\n",
        "def print_keywords_rake(idx, keywords, data, ans):\n",
        "    ans = [preprocess(a) for a in ans[:len(keywords)]]\n",
        "    print(\"\\n=====Abstract=====\")\n",
        "    print(data[idx]['text'])\n",
        "    print(\"\\n===Keywords===\")\n",
        "    for k in keywords:\n",
        "        if k[1] in ans:\n",
        "          print(\"ðŸŸ¢\",end=\"\")\n",
        "        print(k[1],k[0])\n",
        "    print(\"\\n===Keywords by Author===\")\n",
        "    ext_keys = [i[1] for i in keywords]\n",
        "    for i in ans:\n",
        "      if i in ext_keys:\n",
        "          print(\"ðŸ™†â€â™‚ï¸\",end=\"\")\n",
        "      print(i)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvXUCuiuHAf1",
        "outputId": "dcf1a4bf-2d5e-4cc7-c738-9b314c1f5d7a"
      },
      "source": [
        "sample_idx = 238\n",
        "keywords=get_keywords_rake(data[sample_idx]['text'])\n",
        "print_keywords_rake(sample_idx,keywords, data, keys[sample_idx])\n"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=====Abstract=====\nFractional differentiation in passive vibration control\nFrom a single-degree-of-freedom model used to illustrate the concept of\n\tvibration isolation, a method to transform the design for a suspension\n\tinto a design for a robust controller is presented. Fractional\n\tdifferentiation is used to model the viscoelastic behaviour of the\n\tsuspension. The use of fractional differentiation not only permits\n\toptimisation of just four suspension parameters, showing the\n\t'compactness' of the fractional derivative operator, but also leads to\n\trobustness of the suspension's performance to uncertainty of the sprung\n\tmass. As an example, an engine suspension is studied\n\n\n===Keywords===\nðŸŸ¢passive vibration control 8.5\nfractional derivative operator 8.5\nfour suspension parameters 8.0\nfreedom model used 7.0\nðŸŸ¢vibration isolation 4.5\nðŸŸ¢fractional differentiation 4.5\nðŸŸ¢viscoelastic behaviour 4.0\nðŸŸ¢sprung mass 4.0\nðŸŸ¢robust controller 4.0\npermits optimisation 4.0\n\n===Keywords by Author===\nðŸ™†â€â™‚ï¸fractional differentiation\nðŸ™†â€â™‚ï¸passive vibration control\nðŸ™†â€â™‚ï¸vibration isolation\nsuspension\nðŸ™†â€â™‚ï¸robust controller\nðŸ™†â€â™‚ï¸viscoelastic behaviour\nðŸ™†â€â™‚ï¸sprung mass\nengine suspension\ndamping\ndifferentiation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VstXC0bWMu0Q"
      },
      "source": [
        "Here we are! With this method, **six** of extracted top ten keywords are in keywords set selected by author. This is a great work!\n",
        "\n",
        "According to the [RAKE's paper](https://www.researchgate.net/publication/227988510_Automatic_Keyword_Extraction_from_Individual_Documents), RAKE\n",
        "achieves higher precision and similar recall in comparison to existing techniques. RAKE takes a simple set of input parameters and automatically extracts keywords in a single pass, making it suitable for a wide\n",
        "range of documents and collections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n=====Abstract=====\nFractional differentiation in passive vibration control\nFrom a single-degree-of-freedom model used to illustrate the concept of\n\tvibration isolation, a method to transform the design for a suspension\n\tinto a design for a robust controller is presented. Fractional\n\tdifferentiation is used to model the viscoelastic behaviour of the\n\tsuspension. The use of fractional differentiation not only permits\n\toptimisation of just four suspension parameters, showing the\n\t'compactness' of the fractional derivative operator, but also leads to\n\trobustness of the suspension's performance to uncertainty of the sprung\n\tmass. As an example, an engine suspension is studied\n\n\n===Keywords===\nðŸŸ¢passive vibration control 8.5\nfractional derivative operator 8.5\nfour suspension parameters 8.0\nfreedom model used 7.0\nðŸŸ¢vibration isolation 4.5\nðŸŸ¢fractional differentiation 4.5\nðŸŸ¢viscoelastic behaviour 4.0\nðŸŸ¢sprung mass 4.0\nðŸŸ¢robust controller 4.0\npermits optimisation 4.0\n\n===Keywords by Author===\nðŸ™†â€â™‚ï¸fractional differentiation\nðŸ™†â€â™‚ï¸passive vibration control\nðŸ™†â€â™‚ï¸vibration isolation\nsuspension\nðŸ™†â€â™‚ï¸robust controller\nðŸ™†â€â™‚ï¸viscoelastic behaviour\nðŸ™†â€â™‚ï¸sprung mass\nengine suspension\ndamping\ndifferentiation\n"
          ]
        }
      ],
      "source": [
        "sample_idx = 238\n",
        "keywords=get_keywords_rake(data[sample_idx]['text'])\n",
        "print_keywords_rake(sample_idx,keywords, data, keys[sample_idx])"
      ]
    },
    {
      "source": [
        "## Evaluation\n",
        "\n",
        "We use Mean Average Precision(MAP) and Mean Reciprocal Rank(MRR) value as evaluation matrics.\n",
        "\n",
        "[[How to calculate MAP and MRR]](https://medium.com/gumgum-tech/exploring-different-keyword-extractors-evaluation-metrics-and-strategies-ef874d336773)"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "def MAPscore(y_true, y_pred):\n",
        "    rel = 0\n",
        "    pre = []\n",
        "    for i in range(len(y_pred)):\n",
        "        if y_pred[i] in y_true:\n",
        "            rel += 1\n",
        "            pre.append(rel/(i+1))\n",
        "    if rel>0:\n",
        "        return sum(pre)/len(pre)\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "def MRRscore(y_true, y_pred):\n",
        "    for i in range(len(y_pred)):\n",
        "        if y_pred[i] in y_true:\n",
        "            break\n",
        "    return 1/(i+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500\n"
          ]
        }
      ],
      "source": [
        "test_num = 500\n",
        "test_sample_idx = list(range(2000-test_num,2000))\n",
        "print(len(test_sample_idx))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'keys' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'keys' is not defined"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "MAPscores_rake = []\n",
        "MRRscores_rake = []\n",
        "\n",
        "for idx in test_sample_idx:\n",
        "    y_true = keys[idx]\n",
        "    keywords = get_keywords_rake(data[idx]['text'])\n",
        "    y_pred = [i[1] for i in keywords]\n",
        "    MAPscores_rake.append(MAPscore(y_true,y_pred))\n",
        "    MRRscores_rake.append(MRRscore(y_true,y_pred))\n",
        "sum(MAPscores_rake)/test_num,sum(MRRscores_rake)/test_num"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.78 s, sys: 46.4 ms, total: 2.82 s\nWall time: 3.11 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.18731349206349196, 0.25200000000000006)"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ],
      "source": [
        "%%time\n",
        "MAPscores_tfidf = []\n",
        "MRRscores_tfidf = []\n",
        "for idx in test_sample_idx:\n",
        "    y_true = keys[idx]\n",
        "    keywords = get_keywords(idx, docs)\n",
        "    y_pred = [i for i in keywords]\n",
        "    MAPscores_tfidf.append(MAPscore(y_true,y_pred))\n",
        "    MRRscores_tfidf.append(MRRscore(y_true,y_pred))\n",
        "sum(MAPscores_tfidf)/test_num,sum(MRRscores_tfidf)/test_num"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}